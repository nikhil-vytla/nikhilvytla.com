---
title: "Causal Representation Learning"
date: '2025-01-15'
lang: en
duration: 10min
---

[[toc]]

## Introduction & Motivation

What is causal representation learning? It involves methods that aim not to extract low-dimensional features, but to encode the causal factors of variation underlying observed data. To move beyond distributional robustness, we seek models that can reason under interventions and support counterfactual queries. This requires mathematical tools from causal inference and practical algorithms to discover disentangled, **causally-meaningful** representations.

## Generative Model & SCM Formalization

Let $\mathbf{x} \in \mathbb{R}^D$ be an observed datapoint. Assume that $\mathbf{x}$ is generated by underlying causal variables $\mathbf{z} = [z_1, ..., z_K]$. Under a Structural Causal Model (SCM):

$$
z_j = f_j(\mathrm{PA}_j, n_j), \quad \text{where} ~ n_j ~\text{i.i.d. noise}, \; \mathrm{PA}_j=\text{parents of }z_j
\mathbf{x} = g(\mathbf{z}, n_x)
$$

Goal: Find an encoder $E: \mathbf{x} \mapsto \mathbf{\hat{z}}$ s.t. $\mathbf{\hat{z}}$ is (approximately) causally correct and disentangled. We would like

$$
\text{do}(\hat{z}_k = c) \implies \text{systematic change in observation corresponding to } z_k
$$

## Common Approaches & Algorithms

- `FactorVAE`, `BetaVAE`: Encourage disentanglement via penalizing total correlation of $q(\mathbf{z}|\mathbf{x})$.
- `CausalVAE`: Explicit structure priors enforce relations between $z_i$ reflecting causal graph.
- `Invariant Risk Minimization (IRM)`: Learn features $\Phi(\mathbf{x})$ such that $\arg\min_{w} \sum_{e\in \mathcal{E}} R^e(w \circ \Phi)$ yields an invariant predictor across environments.

```python
# Example: FactorVAE loss (PyTorch-like pseudocode)
def factor_vae_loss(x, encoder, decoder, discriminator):
    z_mu, z_logvar = encoder(x)
    z = reparameterize(z_mu, z_logvar)
    recon_x = decoder(z)
    recon_loss = F.mse_loss(recon_x, x)
    tc = total_correlation(z, discriminator)
    return recon_loss + beta * tc
```

## Key Concepts & Equations

- **Total Correlation**: $\mathrm{TC}(\mathbf{z}) = \mathrm{KL}(q(\mathbf{z}) \,\Vert\, \prod_j q(z_j))$ (measures independence of latent dimensions)
- **Identifiability**: Under certain conditions (e.g., multiple environments, known interventions), causal features are learnable.
- **Structural Hamming Distance**: For causal graph evaluation, $\mathrm{SHD}(G_1,G_2)$ counts mismatched edges between graphs.

**Mathematical Example**:

```python
# SCM causal simulation
def scm_sim():
    x1 = np.random.normal()
    x2 = 2*x1 + np.random.normal()
    x3 = x2 - x1 + np.random.normal()
    return np.stack([x1, x2, x3])
```

This code simulates a simple, linear SCM. Changing ("intervening on") $x_1$ propagates through the system.

## Evaluation Metrics

- **Modularity**: Measure sensitivity of each dimension $z_j$ to changes in data or interventions.
- **Disentanglement**: Metrics like DCI and Mutual Information Gap (MIG).

## Open Challenges

- Proving identifiability with finite data.
- Efficient algorithms for high-dimensional, non-linear SCMs.
- Bridging deep learning and graphical model causal theory.

## References

1. [Locatello et al. (2019). Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations](https://arxiv.org/abs/1811.12359)
2. [Sch√∂lkopf et al. (2021). Towards Causal Representation Learning](https://arxiv.org/abs/2102.11107)
3. [Subramanian et al. (2022) Learning Latent Structural Causal Models](https://arxiv.org/abs/2210.13583)
